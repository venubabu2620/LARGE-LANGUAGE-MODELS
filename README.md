# LARGE-LANGUAGE-MODELS
Sentiment analysis on Twitter data using the TweetEval ‚Äì Sentiment dataset.
üìå Project Overview

This project investigates sentiment analysis on Twitter data using the TweetEval ‚Äì Sentiment dataset.
The goal is to compare a traditional machine learning baseline with a medium-sized Large Language Model (BERT) using different fine-tuning strategies and to evaluate their performance, efficiency, and practicality.

The study focuses on:
	‚Ä¢	Understanding the impact of fine-tuning
	‚Ä¢	Comparing full fine-tuning vs parameter-efficient LoRA
	‚Ä¢	Evaluating the effect of hyperparameter tuning
	‚Ä¢	Selecting the best model based on performance and efficiency

‚∏ª

üß† Task Description
	‚Ä¢	Task: Sentiment Classification
	‚Ä¢	Classes:
	‚Ä¢	Negative
	‚Ä¢	Neutral
	‚Ä¢	Positive

This is a multi-class text classification problem on short, informal Twitter text.

‚∏ª

üìä Dataset

Dataset: TweetEval ‚Äì Sentiment
	‚Ä¢	Public benchmark for Twitter NLP tasks
	‚Ä¢	Pre-split into train, validation, and test sets
	‚Ä¢	Tweets are short, informal, and slightly class-imbalanced (Neutral dominates)

üìå Due to class imbalance, macro-averaged Precision, Recall, and F1-score are used for evaluation.

‚∏ª

üèóÔ∏è Models Implemented

1Ô∏è‚É£ Baseline Model
	‚Ä¢	TF-IDF vectorisation (uni-grams + bi-grams)
	‚Ä¢	Logistic Regression
	‚Ä¢	Serves as a reference to quantify improvements from transformer-based models

2Ô∏è‚É£ BERT (No Fine-Tuning)
	‚Ä¢	Pre-trained BERT evaluated directly on the task
	‚Ä¢	Demonstrates that task-specific adaptation is necessary

3Ô∏è‚É£ Fully Fine-Tuned BERT
	‚Ä¢	BERT-base model
	‚Ä¢	All transformer parameters updated
	‚Ä¢	Training and validation loss recorded
	‚Ä¢	Learning curves analysed for convergence and overfitting

4Ô∏è‚É£ LoRA Fine-Tuned BERT
	‚Ä¢	Parameter-efficient fine-tuning using Low-Rank Adaptation (LoRA)
	‚Ä¢	Only small adapter layers trained
	‚Ä¢	Base BERT weights remain frozen
	‚Ä¢	Significantly fewer trainable parameters

‚∏ª

‚öôÔ∏è Hyperparameter Tuning
	‚Ä¢	Hyperparameter optimisation performed using Optuna
	‚Ä¢	Tuned parameters include:
	‚Ä¢	Learning rate
	‚Ä¢	Weight decay
	‚Ä¢	Batch size
	‚Ä¢	Applied to:
	‚Ä¢	Fully fine-tuned BERT
	‚Ä¢	LoRA fine-tuned BERT

This ensures a fair and optimised comparison.

‚∏ª

üìà Evaluation Metrics

All models are evaluated on the held-out test set using:
	‚Ä¢	Accuracy
	‚Ä¢	Macro-averaged Precision
	‚Ä¢	Macro-averaged Recall
	‚Ä¢	Macro-averaged F1-score

Additional analysis includes:
	‚Ä¢	Learning curves (training vs validation loss)
	‚Ä¢	Confusion matrices
	‚Ä¢	Metric comparison bar charts

‚∏ª

üèÜ Key Results

Effect of Fine-Tuning
	‚Ä¢	Baseline model performs moderately but struggles with nuanced sentiment
	‚Ä¢	BERT without fine-tuning performs poorly
	‚Ä¢	Fine-tuned BERT models significantly outperform both baselines

Full Fine-Tuning vs LoRA
	‚Ä¢	Fully fine-tuned BERT achieves the highest macro F1-score
	‚Ä¢	LoRA fine-tuned BERT performs very close to full fine-tuning
	‚Ä¢	LoRA uses far fewer trainable parameters and is more computationally efficient

Learning Curve Analysis
	‚Ä¢	Stable reduction in training and validation loss
	‚Ä¢	No significant overfitting observed
	‚Ä¢	Performance plateaus after a small number of epochs

‚∏ª

‚úÖ Best Model Decision
	‚Ä¢	Best Performance: Fully fine-tuned BERT (after hyperparameter tuning)
	‚Ä¢	Best Efficiency: LoRA fine-tuned BERT (after hyperparameter tuning)
	‚Ä¢	Best Trade-Off: LoRA fine-tuned BERT, due to near-identical performance with much lower computational cost

‚∏ª

‚ö†Ô∏è Limitations
	‚Ä¢	Limited hyperparameter search space
	‚Ä¢	Short training duration
	‚Ä¢	Single dataset (TweetEval only)
	‚Ä¢	No cross-domain or multilingual evaluation

‚∏ª

üîÆ Future Work
	‚Ä¢	Larger hyperparameter search
	‚Ä¢	Evaluate stronger transformer models (RoBERTa, DeBERTa)
	‚Ä¢	Explore different LoRA ranks
	‚Ä¢	Test generalisation on other Twitter datasets

‚∏ª

üìö References
	1.	Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. (2019).
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
https://arxiv.org/abs/1810.04805
	2.	Barbieri, F., Camacho-Collados, J., Espinosa-Anke, L. and Neves, L. (2020).
TweetEval: Unified Benchmark for Tweet Classification.
https://arxiv.org/abs/2010.12421
	3.	Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W. (2021).
LoRA: Low-Rank Adaptation of Large Language Models.
https://arxiv.org/abs/2106.09685
